{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-06-22T11:54:11.785406Z",
     "start_time": "2024-06-22T11:54:11.783272Z"
    }
   },
   "source": "# Create a model that will predict the emotion based on audio",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-22T12:00:04.518052Z",
     "start_time": "2024-06-22T12:00:04.502845Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os, glob, pickle, librosa\n",
    "import numpy as np\n",
    "import soundfile\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import librosa.display\n",
    "import IPython.display as ipd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, classification_report,accuracy_score,f1_score,precision_score,recall_score,roc_auc_score,roc_curve,auc,classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV,cross_val_score\n"
   ],
   "id": "e1004523cd27b90a",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# Load the data",
   "id": "cc0acfb630dd0554"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-22T12:06:33.604296Z",
     "start_time": "2024-06-22T12:06:33.600612Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Extract features (mfcc, chroma, mel) from a sound file\n",
    "def extract_feature(file_name, mfcc, chroma, mel):\n",
    "    with soundfile.SoundFile(file_name) as sound_file:\n",
    "        X = sound_file.read(dtype=\"float32\")\n",
    "        sample_rate=sound_file.samplerate\n",
    "        if chroma:\n",
    "            stft=np.abs(librosa.stft(X))\n",
    "        result=np.array([])\n",
    "        if mfcc:\n",
    "            mfccs=np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=40).T, axis=0)\n",
    "            result=np.hstack((result, mfccs))\n",
    "        if chroma:\n",
    "            chroma=np.mean(librosa.feature.chroma_stft(S=stft, sr=sample_rate).T,axis=0)\n",
    "            result=np.hstack((result, chroma))\n",
    "        if mel:\n",
    "            mel=np.mean(librosa.feature.melspectrogram(y=X, sr=sample_rate).T,axis=0)\n",
    "            result=np.hstack((result, mel))\n",
    "    return result"
   ],
   "id": "6c8a8d924edcd895",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-22T12:06:34.735921Z",
     "start_time": "2024-06-22T12:06:34.733539Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Emotions in the RAVDESS dataset\n",
    "emotions={\n",
    "  '01':'neutral',\n",
    "  '02':'calm',\n",
    "  '03':'happy',\n",
    "  '04':'sad',\n",
    "  '05':'angry',\n",
    "  '06':'fearful',\n",
    "  '07':'disgust',\n",
    "  '08':'surprised'\n",
    "}\n",
    "\n",
    "#Emotions to observe\n",
    "observed_emotions=['neutral','calm', 'happy','sad','angry','fearful', 'disgust','surprised']"
   ],
   "id": "30769cd9036eb8da",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-22T12:06:35.535409Z",
     "start_time": "2024-06-22T12:06:35.532784Z"
    }
   },
   "cell_type": "code",
   "source": "# Load the data and extract features for each sound file",
   "id": "ca118fa3dc7c8d4a",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-22T12:06:35.861742Z",
     "start_time": "2024-06-22T12:06:35.858603Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "def load_data(test_size=0.2):\n",
    "    x,y=[],[]\n",
    "    for file in glob.glob(\"/home/hgidea/Desktop/Coding/Python/internship/null/real_internship/emotion_detection/speech_emotion_detection/sound1/Actor_*/*.wav\"):\n",
    "        file_name=os.path.basename(file)\n",
    "        emotion=emotions[file_name.split(\"-\")[2]]\n",
    "        if emotion not in observed_emotions:\n",
    "            continue\n",
    "        feature=extract_feature(file, mfcc=True, chroma=True, mel=True)\n",
    "        x.append(feature)\n",
    "        y.append(emotion)\n",
    "    return train_test_split(np.array(x), y, test_size=test_size, random_state=9)"
   ],
   "id": "710b7573532b5e1d",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-22T12:06:36.468098Z",
     "start_time": "2024-06-22T12:06:36.465516Z"
    }
   },
   "cell_type": "code",
   "source": "# Split the dataset ",
   "id": "aebb422b09307577",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-22T12:07:58.967703Z",
     "start_time": "2024-06-22T12:06:36.898109Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "x_train,x_test,y_train,y_test=load_data(test_size=0.25)"
   ],
   "id": "199ec36e182abece",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hgidea/Desktop/Coding/Python/.venv/lib/python3.12/site-packages/librosa/core/spectrum.py:266: UserWarning: n_fft=2048 is too large for input signal of length=2\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "all the input arrays must have same number of dimensions, but the array at index 0 has 1 dimension(s) and the array at index 1 has 2 dimension(s)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[16], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m x_train,x_test,y_train,y_test\u001B[38;5;241m=\u001B[39m\u001B[43mload_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtest_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0.25\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[14], line 8\u001B[0m, in \u001B[0;36mload_data\u001B[0;34m(test_size)\u001B[0m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m emotion \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m observed_emotions:\n\u001B[1;32m      7\u001B[0m     \u001B[38;5;28;01mcontinue\u001B[39;00m\n\u001B[0;32m----> 8\u001B[0m feature\u001B[38;5;241m=\u001B[39m\u001B[43mextract_feature\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfile\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmfcc\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mchroma\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[1;32m      9\u001B[0m x\u001B[38;5;241m.\u001B[39mappend(feature)\n\u001B[1;32m     10\u001B[0m y\u001B[38;5;241m.\u001B[39mappend(emotion)\n",
      "Cell \u001B[0;32mIn[11], line 11\u001B[0m, in \u001B[0;36mextract_feature\u001B[0;34m(file_name, mfcc, chroma, mel)\u001B[0m\n\u001B[1;32m      9\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m mfcc:\n\u001B[1;32m     10\u001B[0m     mfccs\u001B[38;5;241m=\u001B[39mnp\u001B[38;5;241m.\u001B[39mmean(librosa\u001B[38;5;241m.\u001B[39mfeature\u001B[38;5;241m.\u001B[39mmfcc(y\u001B[38;5;241m=\u001B[39mX, sr\u001B[38;5;241m=\u001B[39msample_rate, n_mfcc\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m40\u001B[39m)\u001B[38;5;241m.\u001B[39mT, axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)\n\u001B[0;32m---> 11\u001B[0m     result\u001B[38;5;241m=\u001B[39m\u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mhstack\u001B[49m\u001B[43m(\u001B[49m\u001B[43m(\u001B[49m\u001B[43mresult\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmfccs\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     12\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m chroma:\n\u001B[1;32m     13\u001B[0m     chroma\u001B[38;5;241m=\u001B[39mnp\u001B[38;5;241m.\u001B[39mmean(librosa\u001B[38;5;241m.\u001B[39mfeature\u001B[38;5;241m.\u001B[39mchroma_stft(S\u001B[38;5;241m=\u001B[39mstft, sr\u001B[38;5;241m=\u001B[39msample_rate)\u001B[38;5;241m.\u001B[39mT,axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)\n",
      "File \u001B[0;32m~/Desktop/Coding/Python/.venv/lib/python3.12/site-packages/numpy/core/shape_base.py:357\u001B[0m, in \u001B[0;36mhstack\u001B[0;34m(tup, dtype, casting)\u001B[0m\n\u001B[1;32m    355\u001B[0m \u001B[38;5;66;03m# As a special case, dimension 0 of 1-dimensional arrays is \"horizontal\"\u001B[39;00m\n\u001B[1;32m    356\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m arrs \u001B[38;5;129;01mand\u001B[39;00m arrs[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mndim \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[0;32m--> 357\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_nx\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconcatenate\u001B[49m\u001B[43m(\u001B[49m\u001B[43marrs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdtype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcasting\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcasting\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    358\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    359\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _nx\u001B[38;5;241m.\u001B[39mconcatenate(arrs, \u001B[38;5;241m1\u001B[39m, dtype\u001B[38;5;241m=\u001B[39mdtype, casting\u001B[38;5;241m=\u001B[39mcasting)\n",
      "\u001B[0;31mValueError\u001B[0m: all the input arrays must have same number of dimensions, but the array at index 0 has 1 dimension(s) and the array at index 1 has 2 dimension(s)"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# Get the shape of the training and testing datasets",
   "id": "a044910053208515"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "print((x_train.shape[0], x_test.shape[0]))"
   ],
   "id": "9c4e7c5b0cc860e5"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
